{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da95677",
   "metadata": {},
   "source": [
    "# بسم الله الرحمن الرحیم"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7296867",
   "metadata": {},
   "source": [
    "# محمد مهدی شفیقی - پروژه نهایی درس مباحث ویژه"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189a2b0",
   "metadata": {},
   "source": [
    "# تحلیل شبکه همکاری علمی CA-HepTh\n",
    "\n",
    "این پروژه به تحلیل ویژگی‌های ساختاری و دینامیکی شبکه هم‌نویسندگی در حوزه فیزیک انرژی بالا می‌پردازد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f522e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# فقط در صورت نیاز اجرا کن\n",
    "!pip install --quiet networkx pandas numpy matplotlib python-louvain tqdm scipy\n",
    "# برای عملکرد بهتر (اختیاری، سریع‌تر و مقیاس‌پذیرتر)\n",
    "!pip install --quiet python-igraph leidenalg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7976dd41",
   "metadata": {},
   "source": [
    "## تنظیم مسیر داده — مسیر را طبق محل واقعی تغییر بده\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410cd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data/cit-HepTh-abstracts\"   # محتوای پوشه: 1992/, 1993/, ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b133e07",
   "metadata": {},
   "source": [
    "# ایمپورت‌های پایه\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5cec70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, gc, pickle, math, time\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9169c17",
   "metadata": {},
   "source": [
    "# توابع مفید\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e4d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6432abd",
   "metadata": {},
   "source": [
    "# چک محیط و نمونه‌برداری فایل‌ها + تابع استخراج نویسنده (اجرای اولیه)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c24d4",
   "metadata": {},
   "source": [
    "هدف این بلوک این است که بفهمیم ساختار فایل‌های .abs چگونه است، تعداد فایل‌ها در هر پوشه چقدر است، و یک تابع استخراجِ نویسندهٔ مقاوم/قوی بنویسیم که روی دادهٔ واقعی کار کند — بعد از اجرای این بلوک به‌راحتی می‌توانیم بقیهٔ پردازش (ساخت گراف زمانی) را ایمن اجرا کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a1665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found year-folders (sample): ['1992', '1993', '1994', '1995', '1996', '1997']  ... total: 12\n",
      "Total .abs files: 29555\n",
      "Files per year (first 10):\n",
      "  1992: 1367 files\n",
      "  1993: 2058 files\n",
      "  1994: 2377 files\n",
      "  1995: 2303 files\n",
      "  1996: 2606 files\n",
      "  1997: 2673 files\n",
      "  1998: 2758 files\n",
      "  1999: 2803 files\n",
      "  2000: 3126 files\n",
      "  2001: 3153 files\n",
      "\n",
      "Sample files and extracted authors:\n",
      "\n",
      "Year 1992 | file: 9201001.abs\n",
      " Extracted authors: ['C. Itzykson', 'J.-B. Zuber']\n",
      "\n",
      "Year 1993 | file: 9301001.abs\n",
      " Extracted authors: ['G.K.Savvidy', 'K.G.Savvidy']\n",
      "\n",
      "Year 1994 | file: 9401001.abs\n",
      " Extracted authors: ['Jorge Ananias Neto']\n",
      "\n",
      "Year 1995 | file: 9501001.abs\n",
      " Extracted authors: []\n",
      "\n",
      "Year 1996 | file: 9601001.abs\n",
      " Extracted authors: []\n",
      "\n",
      "Year 1997 | file: 9701001.abs\n",
      " Extracted authors: ['M. Zyskin We consider d-dimensional Riemanian manifolds which admit d-2 commuting space-like Killing vector fields', 'orthogonal to a surface', 'containing two one-parametric families of light-like curves. The condition of the Ricci tensor to be zero gives Ernst equations for the metric. We write explicitly a family of local solutions of this equations corresponding to arbitrary initial data on two characteristics in terms of a series. These metrics describe scattering of 2 gravitational waves', 'thus we expect they are very interesting. Ernst equations can be written as equations of motion for some 2D Lagrangian', 'which governs fluctuations of the metric', 'constant in the Killing directions. This Lagrangian looks essentially as a 2D chiral field model', 'thus is possibly treatable in the quantum case by standart methods. It is conceivable that it may describe physics of some specially arranged scattering experiment', 'thus giving an insight for 4D gravity', 'not treatable by standart quantum field theory methods. The renormalization flow for our Lagrangian is different from the flow for the unitary chiral field model', 'the difference is essentially due to the fact that here the field is taking values in a non-compact space of symmetric matrices. We investigate the model']\n",
      "\n",
      "Rough IO check: enumerated ~200 files in 0.00s\n",
      "\n",
      "A-0 done. If sample author extraction looks reasonable, reply with 'A-0 OK' and we'll run A-1: build efficient temporal-edge lists (ids + per-year CSV dump).\n"
     ]
    }
   ],
   "source": [
    "# A-0: Environment check + sample parsing of .abs files\n",
    "# اجرا در یک سلول جدید در Jupyter notebook\n",
    "\n",
    "import os, re, time\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    # tqdm اختیاری است؛ اگر نصب نیست، از آن صرفنظر می‌کنیم\n",
    "    def tqdm(x, **_): \n",
    "        return x\n",
    "\n",
    "# مسیرِ پوشه‌ی cit-HepTh-abstracts را بر اساس ساختار تو تنظیم کن:\n",
    "DATA_ROOT = \"../data/cit-HepTh-abstracts\"   # اگر مسیرت متفاوت است این را تغییر بده\n",
    "\n",
    "# 1) شمارش فایل‌ها و نام پوشه‌ها (سال‌ها)\n",
    "years = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])\n",
    "print(\"Found year-folders (sample):\", years[:6], \" ... total:\", len(years))\n",
    "\n",
    "year_file_counts = {}\n",
    "total_files = 0\n",
    "for y in years:\n",
    "    p = os.path.join(DATA_ROOT, y)\n",
    "    files = [f for f in os.listdir(p) if f.endswith('.abs')]\n",
    "    year_file_counts[y] = len(files)\n",
    "    total_files += len(files)\n",
    "\n",
    "print(f\"Total .abs files: {total_files}\")\n",
    "print(\"Files per year (first 10):\")\n",
    "for y in years[:10]:\n",
    "    print(f\"  {y}: {year_file_counts[y]} files\")\n",
    "\n",
    "# 2) Robust author-extraction function (tries چند الگو)\n",
    "_author_patterns = [\n",
    "    re.compile(r'Authors:\\s*(.*?)\\n(?:Title:|Comments:|\\\\\\n|$)', re.DOTALL | re.IGNORECASE),\n",
    "    re.compile(r'Authors:\\s*(.*)', re.IGNORECASE),\n",
    "]\n",
    "\n",
    "def extract_authors_from_text(txt):\n",
    "    \"\"\"\n",
    "    تلاش می کند رشته‌ی نویسندگان را از متن استخراج کند.\n",
    "    بازگشتی: لیست اسامی نویسنده‌ها (هر اسم تمیزشده)\n",
    "    \"\"\"\n",
    "    # بعضی فایل‌ها از backslash \\\\ برای جدا کردن بخش‌ها استفاده می‌کنند، آن‌ها را به newline تبدیل کن\n",
    "    t = txt.replace('\\\\\\n', '\\n').replace('\\\\', '\\n')\n",
    "    authors_text = None\n",
    "    for pat in _author_patterns:\n",
    "        m = pat.search(t)\n",
    "        if m:\n",
    "            authors_text = m.group(1)\n",
    "            break\n",
    "    if not authors_text:\n",
    "        # fallback: خطی جستجو کن\n",
    "        for line in t.splitlines():\n",
    "            if line.strip().lower().startswith(\"authors:\"):\n",
    "                authors_text = line.split(':',1)[1]\n",
    "                break\n",
    "    if not authors_text:\n",
    "        return []  # هیچ نویسنده‌ای پیدا نشد\n",
    "\n",
    "    # پاک‌سازی و جداسازی: \"and\" و کاما و ; را مدنظر قرار بده\n",
    "    authors_text = authors_text.replace('\\n', ' ')\n",
    "    # بعضی فرمت‌ها \"A and B\" دارند\n",
    "    authors_text = re.sub(r'\\sand\\s', ',', authors_text)\n",
    "    # جداکننده‌ها: ',' یا ';' یا ' and '\n",
    "    parts = re.split(r',|;|\\band\\b', authors_text)\n",
    "    authors = []\n",
    "    for p in parts:\n",
    "        p = p.strip()\n",
    "        if not p:\n",
    "            continue\n",
    "        # حذف موارد غیرِ اسم (مثل affiliation داخل پرانتز)\n",
    "        p = re.sub(r'\\(.*?\\)', '', p).strip()\n",
    "        # normalize spaces\n",
    "        p = re.sub(r'\\s+', ' ', p)\n",
    "        authors.append(p)\n",
    "    # unique preserving order\n",
    "    seen = set()\n",
    "    authors_clean = []\n",
    "    for a in authors:\n",
    "        key = a.lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            authors_clean.append(a)\n",
    "    return authors_clean\n",
    "\n",
    "# 3) نمایش چند نمونه فایل و نتیجه‌ی استخراج نویسنده\n",
    "SAMPLES_TO_SHOW = 6\n",
    "sample_files = []\n",
    "for y in years:\n",
    "    d = os.path.join(DATA_ROOT, y)\n",
    "    files = [f for f in os.listdir(d) if f.endswith('.abs')]\n",
    "    if files:\n",
    "        sample_files.append((y, files[:1][0], os.path.join(d, files[0])))\n",
    "    if len(sample_files) >= SAMPLES_TO_SHOW:\n",
    "        break\n",
    "\n",
    "print(\"\\nSample files and extracted authors:\")\n",
    "for y, fname, fpath in sample_files:\n",
    "    try:\n",
    "        with open(fpath, 'r', encoding='utf-8', errors='ignore') as fh:\n",
    "            txt = fh.read(4000)  # فقط 4k اول برای نمایش\n",
    "    except Exception as e:\n",
    "        txt = f\"(error reading: {e})\"\n",
    "    authors = extract_authors_from_text(txt)\n",
    "    print(f\"\\nYear {y} | file: {fname}\")\n",
    "    print(\" Extracted authors:\", authors[:10])\n",
    "\n",
    "# 4) (اختیاری) تست زمان خواندن تعداد نمونه‌ای از فایل‌ها\n",
    "NTEST = 200   # تعداد فایل‌ها که برای سنجش سرعت پردازش بررسی می‌کنیم\n",
    "t0 = time.time()\n",
    "count = 0\n",
    "for y in years:\n",
    "    d = os.path.join(DATA_ROOT, y)\n",
    "    for fn in os.listdir(d):\n",
    "        if not fn.endswith('.abs'):\n",
    "            continue\n",
    "        count += 1\n",
    "        if count > NTEST:\n",
    "            break\n",
    "    if count > NTEST:\n",
    "        break\n",
    "t1 = time.time()\n",
    "print(f\"\\nRough IO check: enumerated ~{min(count, NTEST)} files in {t1-t0:.2f}s\")\n",
    "\n",
    "print(\"\\nA-0 done. If sample author extraction looks reasonable, reply with 'A-0 OK' and we'll run A-1: build efficient temporal-edge lists (ids + per-year CSV dump).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461920d",
   "metadata": {},
   "source": [
    "# ساخت IDها و فایل‌های یال (per-year CSV) — حافظه‌پسند"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa64332",
   "metadata": {},
   "source": [
    "هدف این گام:\n",
    "\n",
    "به هر نویسنده یک شناسه‌ی عددی (int ID) نسبت دهیم (map: author -> id) تا پردازش‌های بعدی سریع و حافظه‌پسند شوند.\n",
    "\n",
    "برای هر سال یک فایل CSV بسازیم که هر سطرش یک یالِ بدون‌جهت (undirected) را به‌صورت src_id,dst_id,year ذخیره کند.\n",
    "\n",
    "یال‌ها در هر سال بدون تکرار (deduped) باشند و جفت‌ها به‌صورت مرتب شده (min_id,max_id) ذخیره شوند تا تکرار‌های (u,v) و (v,u) یکسان شناخته شوند.\n",
    "\n",
    "یک فایل nodes.csv بسازیم که id → author_name نگه دارد (برای مرجع و مصورسازی بعدی)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b44d49cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years found: ['1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003']\n",
      "Year 1992: processed 1367 files -> 1911 unique edges  (time: 2.02s)\n",
      "Year 1993: processed 2058 files -> 2673 unique edges  (time: 2.19s)\n",
      "Year 1994: processed 2377 files -> 3131 unique edges  (time: 1.44s)\n",
      "Year 1995: processed 2303 files -> 3588 unique edges  (time: 1.83s)\n",
      "Year 1996: processed 2606 files -> 4876 unique edges  (time: 1.53s)\n",
      "Year 1997: processed 2673 files -> 3823 unique edges  (time: 1.51s)\n",
      "Year 1998: processed 2758 files -> 4340 unique edges  (time: 1.64s)\n",
      "Year 1999: processed 2803 files -> 4595 unique edges  (time: 1.52s)\n",
      "Year 2000: processed 3126 files -> 4755 unique edges  (time: 1.45s)\n",
      "Year 2001: processed 3153 files -> 5064 unique edges  (time: 1.77s)\n",
      "Year 2002: processed 3312 files -> 6656 unique edges  (time: 1.70s)\n",
      "Year 2003: processed 1019 files -> 2240 unique edges  (time: 0.50s)\n",
      "\n",
      "Done. Total authors (unique): 16715\n",
      "Per-year edge counts: {'1992': 1911, '1993': 2673, '1994': 3131, '1995': 3588, '1996': 4876, '1997': 3823, '1998': 4340, '1999': 4595, '2000': 4755, '2001': 5064, '2002': 6656, '2003': 2240}\n",
      "Nodes file saved to: ../data/edges_by_year\\nodes.csv\n",
      "Per-year CSVs saved to: ../data/edges_by_year\n",
      "Elapsed time: 19.15s\n"
     ]
    }
   ],
   "source": [
    "# A-1: Build temporal edge lists (author -> id map + per-year deduped CSVs)\n",
    "import os, re, time, csv\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "# مسیر ورودی (پوشه cit-HepTh-abstracts) — اگر مسیرت فرق داره اینجا را تغییر بده\n",
    "DATA_ROOT = \"../data/cit-HepTh-abstracts\"\n",
    "OUT_DIR = \"../data/edges_by_year\"   # خروجی: فایل‌های CSV برای هر سال و nodes.csv\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# تابع استخراج نویسنده را از A-0 بیاور (یا از src/data_parser.py)\n",
    "# اگر در نوت بوک قبلی تعریف شده می‌توان مستقیماً استفاده کرد؛ در غیر این صورت این پیاده‌سازی را کپی کن:\n",
    "def extract_authors_from_text(txt):\n",
    "    t = txt.replace('\\\\\\n', '\\n').replace('\\\\', '\\n')\n",
    "    # الگوهای ساده\n",
    "    m = re.search(r'Authors:\\s*(.*?)\\n(?:Title:|Comments:|\\\\\\n|$)', t, re.DOTALL | re.IGNORECASE)\n",
    "    authors_text = None\n",
    "    if m:\n",
    "        authors_text = m.group(1)\n",
    "    else:\n",
    "        # fallback خطی\n",
    "        for line in t.splitlines():\n",
    "            if line.strip().lower().startswith(\"authors:\"):\n",
    "                authors_text = line.split(':',1)[1]\n",
    "                break\n",
    "    if not authors_text:\n",
    "        return []\n",
    "    authors_text = authors_text.replace('\\n', ' ')\n",
    "    authors_text = re.sub(r'\\sand\\s', ',', authors_text)\n",
    "    parts = re.split(r',|;|\\band\\b', authors_text)\n",
    "    authors = []\n",
    "    for p in parts:\n",
    "        p = p.strip()\n",
    "        if not p:\n",
    "            continue\n",
    "        p = re.sub(r'\\(.*?\\)', '', p).strip()\n",
    "        p = re.sub(r'\\s+', ' ', p)\n",
    "        authors.append(p)\n",
    "    seen = set(); out=[]\n",
    "    for a in authors:\n",
    "        k = a.lower()\n",
    "        if k not in seen:\n",
    "            seen.add(k); out.append(a)\n",
    "    return out\n",
    "\n",
    "# 1) scan years (sorted)\n",
    "years = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])\n",
    "print(\"Years found:\", years)\n",
    "\n",
    "# global author -> id mapping (OrderedDict to keep insertion order stable)\n",
    "author2id = OrderedDict()\n",
    "next_id = 0\n",
    "\n",
    "# We'll store for each year a set of edges (tuple (u_id, v_id) with u<v)\n",
    "# For memory-safety: we'll use a set per year (dataset small enough). If memory problem داشتیم،\n",
    "# می‌توانستیم flush به فایل موقت و merge پس از آن انجام دهیم.\n",
    "year_edge_counts = {}\n",
    "start = time.time()\n",
    "\n",
    "for y in years:\n",
    "    year_dir = os.path.join(DATA_ROOT, y)\n",
    "    edge_set = set()\n",
    "    files = [f for f in os.listdir(year_dir) if f.endswith('.abs')]\n",
    "    # progress print\n",
    "    t0 = time.time()\n",
    "    for i,fn in enumerate(files):\n",
    "        fpath = os.path.join(year_dir, fn)\n",
    "        with open(fpath, 'r', encoding='utf-8', errors='ignore') as fh:\n",
    "            txt = fh.read()\n",
    "        authors = extract_authors_from_text(txt)\n",
    "        if len(authors) < 2:\n",
    "            continue\n",
    "        # assign ids\n",
    "        ids = []\n",
    "        for a in authors:\n",
    "            key = a.strip()\n",
    "            if key not in author2id:\n",
    "                author2id[key] = next_id\n",
    "                ids.append(next_id)\n",
    "                next_id += 1\n",
    "            else:\n",
    "                ids.append(author2id[key])\n",
    "        # all pairs (undirected clique)\n",
    "        for u,v in combinations(ids, 2):\n",
    "            if u == v:\n",
    "                continue\n",
    "            a,b = (u,v) if u < v else (v,u)\n",
    "            edge_set.add((a,b))\n",
    "    # write deduped per-year csv\n",
    "    out_csv = os.path.join(OUT_DIR, f\"edges_{y}.csv\")\n",
    "    with open(out_csv, 'w', newline='', encoding='utf-8') as csvf:\n",
    "        writer = csv.writer(csvf)\n",
    "        # header optional\n",
    "        writer.writerow([\"src\",\"dst\",\"year\"])\n",
    "        for (u,v) in sorted(edge_set):\n",
    "            writer.writerow([u,v,y])\n",
    "    year_edge_counts[y] = len(edge_set)\n",
    "    print(f\"Year {y}: processed {len(files)} files -> {year_edge_counts[y]} unique edges  (time: {time.time()-t0:.2f}s)\")\n",
    "\n",
    "# write nodes file\n",
    "nodes_file = os.path.join(OUT_DIR, \"nodes.csv\")\n",
    "with open(nodes_file, 'w', newline='', encoding='utf-8') as nf:\n",
    "    w = csv.writer(nf)\n",
    "    w.writerow([\"id\",\"author\"])\n",
    "    for author,aid in author2id.items():\n",
    "        w.writerow([aid, author])\n",
    "\n",
    "total_time = time.time() - start\n",
    "print(\"\\nDone. Total authors (unique):\", len(author2id))\n",
    "print(\"Per-year edge counts:\", year_edge_counts)\n",
    "print(\"Nodes file saved to:\", nodes_file)\n",
    "print(\"Per-year CSVs saved to:\", OUT_DIR)\n",
    "print(f\"Elapsed time: {total_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c53eee",
   "metadata": {},
   "source": [
    "الان ما یک نسخه تمیز و بهینه از یال‌های زمانی داریم که:\n",
    "\n",
    "۱۶٬۷۱۵ نویسنده یکتا (id → name)\n",
    "\n",
    "یال‌های منحصر به فرد برای هر سال بدون تکرار\n",
    "\n",
    "همه‌چیز آماده برای ساخت snapshot‌ های گراف و تحلیل شاخص‌هاست\n",
    "\n",
    "حالا می‌توانیم وارد A-2 شویم:\n",
    "\n",
    "ساخت گراف برای هر سال از روی CSVها\n",
    "\n",
    "محاسبه شاخص‌های پایه برای هر snapshot (Average Degree, Density, Clustering Coefficient، اندازه مولفه بزرگ، Approx Diameter، Approx Betweenness، PageRank و غیره)\n",
    "\n",
    "ذخیره نتایج در یک جدول (DataFrame) برای استفاده در تحلیل رشد و ترسیم نمودارها"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ddf82",
   "metadata": {},
   "source": [
    "# محاسبات پایه"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cbed64",
   "metadata": {},
   "source": [
    "خواندن CSV همان سال و ساخت گراف با NetworkX\n",
    "\n",
    "محاسبه شاخص‌ها:\n",
    "\n",
    "avg_degree → میانگین درجه\n",
    "\n",
    "density → چگالی گراف\n",
    "\n",
    "num_nodes, num_edges → تعداد گره و یال\n",
    "\n",
    "largest_cc_size → اندازه بزرگ‌ترین مؤلفه همبند\n",
    "\n",
    "avg_clustering → ضریب خوشه‌بندی میانگین\n",
    "\n",
    "approx_diameter → قطر تقریبی با BFS sampling\n",
    "\n",
    "betweenness_approx → بینابینی تقریبی با k گره نمونه\n",
    "\n",
    "pagerank → پیج‌رنک با پیاده‌سازی Sparse Power Iteration که نوشتیم\n",
    "\n",
    "ذخیره همه در یک DataFrame و سیو به CSV برای تحلیل بعدی."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d732ea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.04922032356262207 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, random, time\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# مسیر پوشه CSV های per-year\n",
    "edges_dir = \"../data/edges_by_year\"\n",
    "\n",
    "years = sorted([y for y in os.listdir(edges_dir) if y.endswith(\".csv\") == False and y.isdigit()])\n",
    "\n",
    "results = []\n",
    "\n",
    "def approx_diam(G, k=30):\n",
    "    nodes = list(G.nodes())\n",
    "    max_e = 0\n",
    "    for _ in range(k):\n",
    "        s = random.choice(nodes)\n",
    "        lengths = nx.single_source_shortest_path_length(G, s)\n",
    "        e = max(lengths.values())\n",
    "        if e > max_e:\n",
    "            max_e = e\n",
    "    return max_e\n",
    "\n",
    "def pagerank_power_sparse(G, alpha=0.85, max_iter=100, tol=1e-6):\n",
    "    nodes = list(G.nodes())\n",
    "    n = len(nodes)\n",
    "    idx = {n: i for i, n in enumerate(nodes)}\n",
    "    rows, cols = [], []\n",
    "    for u,v in G.edges():\n",
    "        rows.append(idx[v])\n",
    "        cols.append(idx[u])\n",
    "    data = np.ones(len(rows))\n",
    "    M = sp.csr_matrix((data, (rows, cols)), shape=(n, n), dtype=float)\n",
    "    col_sums = np.array(M.sum(axis=0)).flatten()\n",
    "    inv_col_sums = np.zeros_like(col_sums)\n",
    "    col_nz = col_sums != 0\n",
    "    inv_col_sums[col_nz] = 1.0 / col_sums[col_nz]\n",
    "    D = sp.diags(inv_col_sums)\n",
    "    M = M.dot(D)\n",
    "    x = np.ones(n) / n\n",
    "    teleport = (1.0 - alpha) / n\n",
    "    for _ in range(max_iter):\n",
    "        x_last = x.copy()\n",
    "        x = alpha * (M.dot(x)) + teleport\n",
    "        if np.linalg.norm(x - x_last, 1) < tol:\n",
    "            break\n",
    "    return dict(zip(nodes, x))\n",
    "\n",
    "t0 = time.time()\n",
    "for year in years:\n",
    "    edge_file = os.path.join(edges_dir, year, \"edges.csv\")\n",
    "    df_edges = pd.read_csv(edge_file)\n",
    "    G = nx.from_pandas_edgelist(df_edges, 'src', 'dst')\n",
    "\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    avg_deg = sum(dict(G.degree()).values()) / num_nodes\n",
    "    dens = nx.density(G)\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    G_main = G.subgraph(largest_cc).copy()\n",
    "    largest_cc_size = G_main.number_of_nodes()\n",
    "    avg_clust = nx.average_clustering(G)\n",
    "    diam = approx_diam(G_main, k=20)\n",
    "    betw_approx = nx.betweenness_centrality(G_main, k=50, seed=42)\n",
    "    pr = pagerank_power_sparse(G)\n",
    "\n",
    "    results.append({\n",
    "        \"year\": year,\n",
    "        \"nodes\": num_nodes,\n",
    "        \"edges\": num_edges,\n",
    "        \"avg_degree\": avg_deg,\n",
    "        \"density\": dens,\n",
    "        \"largest_cc_size\": largest_cc_size,\n",
    "        \"avg_clustering\": avg_clust,\n",
    "        \"approx_diameter\": diam,\n",
    "        \"top5_pagerank\": sorted(pr.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"../data/basic_metrics_per_year.csv\", index=False)\n",
    "print(\"Done in\", time.time()-t0, \"seconds\")\n",
    "df_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
